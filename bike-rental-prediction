{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abdalrhamnhebishy/bike-rental-prediction?scriptVersionId=211613961\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from IPython.core.display import HTML\n# Apply styles globally within the notebook\nHTML('''\n<style>\n  h2 {\n    font-size: var(--j3p-content-font-size4);\n    background-color:#33ffc1;\n    color: white;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    padding: 10px 41px;\n    top: 10px;\n    position: relative;\n    border-radius: 10px 50px 10px 50px;\n}\n\n img {       /* flex-grow: 1; */\n            /* flex-shrink: 1; */\n            border-radius: 100px 70px 150px 70px; \n            border: 10px solid #eee;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            transition: box-shadow 0.3s ease; /* Add a transition for a smooth effect */\n        }\n\n       img:active {\n          box-shadow: 0 10px 20px rgba(173, 216, 230, 1) 0 6px 6px rgba(0, 0, 255, 1.0\n173, 216, 230, 1);\n        }\nvideo {\n    width: 820px; /* Set the width of the video */\n    height: auto; /* Adjust the height automatically */\n    border-radius: 25px; /* Add a border radius for rounded corners */\n    border: 5px solid #eee; /* Add a border */\n    box-shadow: 0 10px 20px rgba(173, 216, 230, 1), 0 6px 6px rgba((0, 0, 255, 1.0)); /* Add a box shadow */\n    display: flex;\n    controls: autoplay;\n    align-items: center;\n    justify-content: center;\n    transition: box-shadow 0.3s ease; /* Add a transition for a smooth effect */\n  }\n\n  video:active {\n    box-shadow: 0 10px 20px rgba(173, 216, 230, 1), 0 6px 6px rgba();\n  }\n\n\n</style>\n''')  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:33:52.611607Z","iopub.execute_input":"2024-12-06T19:33:52.612057Z","iopub.status.idle":"2024-12-06T19:33:52.650795Z","shell.execute_reply.started":"2024-12-06T19:33:52.612021Z","shell.execute_reply":"2024-12-06T19:33:52.649337Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bike Rental Prediction :","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://as1.ftcdn.net/v2/jpg/01/17/62/58/1000_F_117625802_os5LYMvHJ20rJWegManCIJGzDn4wkL7O.jpg\" alt=\"My Image\">","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border: 2px solid red; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n   \n    \n    \n<p> &bull;        <a target=\"_blank\"> Introudction</a>  </p> \n\n<p> &bull;      <a target=\"_blank\"> Explanory Data Analysis</a>  </p>\n\n<p>&bull;       <a target=\"_blank\"> Data Processing</a>  </p>\n\n<p>&bull;       <a target=\"_blank\"> Model Bulding</a> </p>   \n    \n<p>&bull;       <a target=\"_blank\"> Evalution Metrics </a> </p> \n\n<p>&bull;       <a target=\"_blank\"> Results & see the best models </a> </p>   \n\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Data Information","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border: 2px solid red; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n\n<p>&bull;The dataset contains count of public bicycles rented per hour in the Seoul Bike Sharing System, with corresponding weather data and holiday information</p> \n\n<p>&bull;\nDataset Characteristics : Multivariate\n\nSubject Area : Business\n\nAssociated Tasks : Regression\n\nFeature :Type  Integer, Real\n\nInstances : 8760 (num of rows)\n\nFeatures : 13 </p>\n\n<p>&bull;Additional Information :\n\nCurrently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort.</p>\n\n<p>&bull; It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time.</p> \n\n<p>&bull; Eventually, providing the city with a stable supply of rental bikes becomes a major concern.</p> \n\n<p>&bull; The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.</p> \n\n<p>&bull; The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information. </p> \n\n<p>&bull; This Data From : Seoul Bike Sharing Demand\nDonated on 2/29/2020</p>   \n\n<p>&bull; <a href=\"https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand\" target=\"_blank\">Data Source</a>.</p>\n\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Understand the problem :\n\n\n\n\n\n\n\n\n<div style=\"border: 2px solid red; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p> &bull; This problem Consider as Regression problrm   .</p> \n\n<p> &bull; Target of this problem to predict on  number of bikes rented per hour and date information.  .</p> \n\n\n<p> &bull; I will Train 5 models for Machine learning with visualization of them. (linearRegression,Random ForestRegressor,Decision Tree Regressor,gradient boosting regressor .</p> \n\n<p>&bull; Also i will Make Explanory Analysis for the Data By using both SQL & Pandas  </p>\n\n<p>&bull; Also i will learn the Relations between the Feature to  Make Different Plots that Represent the Data  </p>   \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"| Data fields      | Description                                                                                                 |\n| ------------- | ----------------------------------------------------------------------------------------------------------- |\n| ID           | an ID for this instance                                                                                      |\n| Date          |  year-month-day                                                                                       |\n| Hour                       |  Hour of he day                                                                             |\n| Temperature          |  Temperature in Celsius                                                                                           |\n| Humidity               | - %                                                                                 |\n| Windspeed        |    - m/s                                                                           |\n| Visibility              | - 10m                                                                                   |\n|Dew point temperature           |  - Celsius                                                                           |\n| Solar radiation   |  - MJ/m2                                                                    |\n|Rainfall           | - mm                                                                   |\n| Seasons             | Winter, Spring, Summer, Autumn                                                                  |                     \n| Holiday            |        - Holiday/No holiday                                                            |    \n|Functional Day         |    - NoFunc(Non Functional Hours), Fun(Functional hours)                                                            | \n|y (Target)      |   - Rented Bike count , Count of bikes rented at each hour                                                          | \n","metadata":{}},{"cell_type":"markdown","source":"## imports ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport graphviz\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder,MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn import tree\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\nimport sqlite3\nfrom plotly.graph_objs import *\nimport plotly.express as px \nimport plotly.graph_objects as go\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings('ignore')\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the data:\n\ndata=pd.read_csv(\"rental_data.csv\")\ndata.head(25)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explore the Data :","metadata":{}},{"cell_type":"code","source":"#get info about the dataset:\nprint(data.info())\nprint(\"-\"*25)\n# to know more about the shape of the dataset:\nprint(\"shape of the dataset -->>\",np.shape(data))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#get the statistical:\ndata.describe().T","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get Types of features:\n\ndata.dtypes","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# know the number of every item in every column:\n\nfor col in data.columns:\n    print(f\"counts of items in {col} -->> \\n {data[col].value_counts()}\")\n    print(\"-\"*25)","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to know the uniques values in evey column in the dataset:\nfor col in data.columns:\n    print(f\"All items in the ---->>>{col} column  :  \\n{data[col].unique()}\")\n    print(\"*\"*50)","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract all Types of columns from the data :\nfor col in data.columns:\n    obj_cols=data.select_dtypes(\"object\").columns\n\n    num_cols=data.select_dtypes(\"number\").columns\n\n    disc_cols=data.select_dtypes(include=(\"int64\")).columns\n\n    conts_cols=data.select_dtypes(include=(\"float64\")).columns\n\n    Nomial_cols=data.nunique()[data.nunique()<3]\n\nprint(f\"object_columns ----->>> {obj_cols}\\n Numerical_columns ----->>> {num_cols} \\n Discrete_columns ---->>> {disc_cols} \\n Continous_cols ----->> {conts_cols} \\n Nominal Columns --->> {Nomial_cols}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cleaning :\n\n\n\n\n\n\n\n<div style=\"border: 3px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p> &bull; At First i checked if ound any Null Values . </p> \n\n<p> &bull; Also i Removed unless columns like ID . </p>\n\n<p> &bull; I checked Duplicates values & Remove it if ound . </p>\n\n<p> &bull; I removed Structure null values like (units in the columns)  </p>\n\n<p> &bull; I used the Simple Imputer Method to fill the Null Values if it was very big . </p> \n\n<p> &bull; I used median Strategy with Numerical Features  .</p> \n\n<p> &bull; I make Reshape , then i make squeeze on it  . </p>\n\n<p> &bull; Any Column , Rate of missing values Reach above 75% ,Delete it  . </p>\n\n</div>\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# check the Null Values :\n# ok there no Any Null values :\ndata.isna().mean()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop any null values if found :\ndf = data.dropna()\ndf.isna().mean()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop unless column like ID :\n\ndf=df.drop(\"ID\",axis=1)\ndf.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if there are aduplicated values in the data & drop it if found :\ndf=data\nduplicated_features=df.duplicated().sum()\nprint(\"Number of duplicates ----->>> \",duplicated_features)\ndf = df.drop_duplicates()\nduplicated_features=df.duplicated().sum()\nprint(\"Number of duplicates of cleaning it ----->>> \",duplicated_features)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove units and other elements from column headers\ndf = data.rename(columns={\n    'Temperature(�C)': 'Temperature',\n    'Humidity(%)': 'Humidity',\n    'Wind speed (m/s)': 'Wind_speed',\n    'Visibility (10m)':'Visibility',\n    'Dew point temperature(�C)':'Dew_point_temperature',\n    'Solar Radiation (MJ/m2)':'Solar_Radiation',\n    'Rainfall(mm)':'Rainfall',\n    'Snowfall (cm)':'Snowfall(cm)'\n    })\n\ndf.head(10)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engeneering :\n\n\n\n\n\n\n\n<div style=\"border: 3px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p> &bull; first : i get the Minimum & Maximum for each feature  </p> \n\n<p> &bull; Then i Divide every feature into Categories base on Range of values  . </p>\n\n<p> &bull; at least , we can say that we doing feature Engeneering with correctly style  . </p>\n\n<p> &bull;ex : Date Divide into Day , Month , Year </p>\n\n<p> &bull; other Features into Categories (very low , low , monderate , week mondarate , High,.....). </p> \n\n\n</div>\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# to know the min & max value at each feaure :\n\nfor col in df.columns :\n    print(f\"maximum value at {col} = {np.max(df[col])} & minimum value = {np.min(df[col])} \")\n    print(\"-\"*50)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make feature engeneering for visibility_value :\n\ndef get_visibility_condition(visibility_value):\n    if visibility_value < 50:return 1   # Poor\n        \n    elif 50 <= visibility_value < 500:return 2   # Moderate\n        \n    elif 500 <= visibility_value < 1000:return 3 # Good\n        \n    elif visibility_value >= 1000:return 4     # Excellent\n    else:return 0   # unknown \ndf['Visibility'] = df['Visibility'].apply(get_visibility_condition)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Divide Date into Day , month and year : \n\ndf['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n\n# New Columns \ndf['Day'] = df['Date'].dt.day\ndf['Month'] = df['Date'].dt.month\ndf['Year'] = df['Date'].dt.year\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Divide Temprature based on ranges of values into categories :\n\ndef get_temperature_condation(temperature):\n    if temperature < -15.1: return 1            # Extremely Cold\n    elif -15.1 <= temperature < 0: return 2    # Very Cold\n    elif 0 <= temperature < 10: return 3        # Cold\n    elif 10 <= temperature < 20:return  4       # Cool\n    elif 20 <= temperature < 30:return 5      # Moderate\n    elif 30 <= temperature < 40:return  6       # Warm\n    elif 40 <= temperature <= 50:return  7      # Hot\n    else:return  8                            #  Extreme\ndf[\"Temperature\"]=df[\"Temperature\"].apply(get_temperature_condation)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Divide himidity based on ranges of values into categories :\n\ndef get_humidity_condation(humidity):\n    if humidity < 20: return  1           # Very Dry\n    elif 20 <= humidity < 40: return 2      # Dry\n    elif 40 <= humidity < 60:  return 3   # Moderate\n    elif 60 <= humidity < 80: return 4       # Humid\n    elif 80 <= humidity <= 98: return 5      # Very Humid\n    else: return 6        # Extreme\n\ndf[\"Humidity\"]=df[\"Humidity\"].apply(get_humidity_condation) \n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Divide Wind_speed based on ranges of values into categories  :\n\ndef get_Wind_speed_condition(Wind_speed):\n    if Wind_speed < 1: return  1               # Very Low\n    elif 1 <= Wind_speed < 2: return 2      # Low\n    elif 2 <= Wind_speed < 3: return 3       # Moderate Low\n    elif 3 <= Wind_speed < 4: return 4     # Moderate\n    elif 4 <= Wind_speed < 5: return  5         # Moderate High\n    elif 5 <= Wind_speed < 6: return  6       #High\n    elif 6 <= Wind_speed <= 7.4: return 7      #Very High\n    else: return 8      # Extreme\n\ndf[\"Wind_speed\"]=df[\"Wind_speed\"].apply(get_Wind_speed_condition) \n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Divide Solar Radition into categories based on Range of values : \n\ndef get_solar_radiation(solar_radiation):\n    if solar_radiation < 0.5: return 1     # Very Week\n    elif 0.5 <= solar_radiation < 1.0:return  2   # Week\n    elif 1.0 <= solar_radiation < 2.0:return 3    # Moderate Week\n    elif 2.0 <= solar_radiation < 3.0:return  4    # Moderate\n    elif 3.0 <= solar_radiation <= 3.52:return 5    # Strong\n    else:return  6  # Extreme\n\ndf['Solar_Radiation'] = df['Solar_Radiation'].apply(get_solar_radiation)\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Divide rainfall into categories based on the range of it values : \n\ndef get_rainfall(rainfall):\n    if rainfall < 5:return 1                 # Very Week\n    elif 5 <= rainfall < 10:return 2          # Week\n    elif 10 <= rainfall < 15:return 3        # Moderate Week\n    elif 15 <= rainfall < 20:return 4         # Moderate\n    elif 20 <= rainfall < 25:return  5         # Moderate High\n    elif 25 <= rainfall < 30:return 6           # High\n    elif 30 <= rainfall <= 35:return 7        # Very High\n    else:return  8  #Extreme\n\n\ndf['Rainfall'] = df['Rainfall'].apply(get_rainfall)\nprint(df)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explanory on Dataset: \n\n\n\n\n<div style=\"border: 5px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p> &bull; Contex : </p> \n\n<p> &bull; Correlation : I make correlation to study Relations between features </p> \n\n<p> &bull; outliers : Defination outliers and check on each feature   .</p> \n<p> &bull; outliers : see percentage of every outliers at any column and how dealing with it    .</p> \n\n<p> &bull; heatmap : very strong plot that show the Strong / week Relations between features   .</p> \n</div>","metadata":{}},{"cell_type":"code","source":"# Calculate the correlation :\n\ncorr=df.select_dtypes(\"number\").corr()\ncorr","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot the heatmap:\n# plot the heatmap_Correlation_MatRix:\n\nf1={\"family\":\"serif\",\"size\":25,\"color\":\"b\"}\nplt.figure(figsize=(25,25),dpi=120)\nsns.heatmap(df.select_dtypes(\"number\").corr(),annot=True,fmt=\"0.2f\",cmap='summer',linewidths=0.5)\nplt.xticks(rotation=45,color=\"b\")\nplt.yticks(rotation=-45,color=\"b\")\nplt.xlabel(\"features\",fontdict=f1)\nplt.ylabel(\"features\",fontdict=f1)\nplt.title(\"Correlation_heatmap_Matrix\",fontdict=f1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defination Outliers :","metadata":{}},{"cell_type":"code","source":"# found many feature contain the outliers  :\nfeatures=df.select_dtypes(include=\"number\").columns\n\nfor col in features:\n    Q1_col,Q3_col=df[col].quantile([0.25,0.75])\n    iqr=Q3_col-Q1_col\n    low_limit=Q1_col-1.5*iqr\n    upper_limit=Q3_col+1.5*iqr\n    outlier=[]\n    for x in df[col]:\n         if ((x> upper_limit) or (x<low_limit)):\n             outlier.append(x)\n    if len(outlier)==0:\n        print(f\" * -- >> there No outlier in {col} feature\")\n    else:\n        print(f\"Ther outlier in this feature {col}\")\n\n    print(f\"Q1 of {col} --->>> {Q1_col} \\n Q3 of {col} ---->>> {Q3_col} \\n iqr--->>{iqr}\\n low_limit--->>>{low_limit} \\n  upper_limit--->>> {upper_limit} \\n outlier---->>> {outlier} \\n Number of outliers --->>> {len(outlier)} \")\n    print(\"-\"*25)\n    ","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from this Represention we discover that we found outliers :\n\nf1={\"family\":\"serif\",\"size\":9,\"color\":\"r\"}\nplt.figure(figsize=(20,20),dpi=250)\nplt.title(\"This Represention to check if found  outliers\",fontdict=f1)\nplt.xlabel(\"Features\",fontdict=f1)\nplt.ylabel(\"count of happens\",fontdict=f1)\nplt.xticks(rotation=45,color=\"b\")\nsns.boxplot(df[features])\nplt.legend(title=\"outliers\",prop={'size': 5})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Some Another Definations :\n\nF=df.select_dtypes(include=\"number\").columns \nnum_columns = len(F)\nplt.figure(figsize=(30,35),dpi=50)\nf2={\"family\":\"serif\",\"size\":20,\"color\":\"r\"}\n\n# Create side-by-side boxplots for each feature :\n\nfor i, feature in enumerate(F):\n    plt.subplot(1, num_columns, i + 1)\n    sns.boxplot(df[feature].dropna(), patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n    plt.ylabel(f'Boxplot of {feature} ',fontdict=f2)\n    plt.xticks(rotation=45, color=\"b\")\n    plt.grid(True)\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split the data int train & test :\n\nx_reg,y_reg=make_regression()\nx_reg=df.drop(\"y\",axis=1)\ny_reg=df[\"y\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_outliers_iqr(df):\n    outlier_counts = {}\n    outlier_means = {}\n    \n    \n    for column in df.select_dtypes(include=[np.number]).columns:  \n        Q1 = df[column].quantile(0.25)\n        Q3 = df[column].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        # Identify outliers\n        \n        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n        \n        outlier_counts[column] = outliers.shape[0]\n        \n        # Calculate mean of outliers if any exist\n        \n        if not outliers.empty:\n            outlier_means[column] = outliers[column].mean()\n        else:\n            outlier_means[column] = 0.0\n        \n        # Remove outliers from the DataFrame\n        \n        df_cleaned_without_target = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n    \n    return df_cleaned_without_target, outlier_counts, outlier_means\n\n\ndf_cleaned_without_target, outlier_counts, outlier_means = remove_outliers_iqr(x_reg)\n\n\n# Display the results\nprint(\"Number of outliers detected in each column:\")\nprint(outlier_counts)\nprint(\"-\" * 70)\nprint(\"\\nMean of outliers in each column:\")\nprint(outlier_means)\nprint(\"-\" * 70)\nprint(\"\\nDataFrame after removing outliers:\")\nprint(df_cleaned_without_target)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## check the outliers after cleaning the data :","metadata":{}},{"cell_type":"markdown","source":"## Checking outliers after cleaning the Data without Target : \n\n\n\n\n<div style=\"border: 5px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p> &bull; After Cleaning Data Without Target : </p> \n\n<p> &bull; we found that : number outliers in columans that was inside it outliers is Reduced </p> \n\n<p> &bull; outliers : For that we will fill this outliers to make the Data Cleaning  .</p> \n\n</div>","metadata":{}},{"cell_type":"code","source":"# found many feature contain the outliers  :\n\nf=df_cleaned_without_target.select_dtypes(include=\"number\").columns\n\nfor col in f:\n    Q1_col,Q3_col=df_cleaned_without_target[col].quantile([0.25,0.75])\n    iqr=Q3_col-Q1_col\n    low_limit=Q1_col-1.5*iqr\n    upper_limit=Q3_col+1.5*iqr\n    outlier=[]\n    for x in df_cleaned_without_target[col]:\n         if ((x> upper_limit) or (x<low_limit)):\n             outlier.append(x)\n    if len(outlier)==0:\n        print(f\" * -- >> there No outlier in {col} feature\")\n    else:\n        print(f\"Ther outlier in this feature {col}\")\n\n    print(f\"Q1 of {col} --->>> {Q1_col} \\n Q3 of {col} ---->>> {Q3_col} \\n iqr--->>{iqr}\\n low_limit--->>>{low_limit} \\n  upper_limit--->>> {upper_limit} \\n outlier---->>> {outlier} \\n Number of outliers --->>> {len(outlier)} \")\n    print(\"-\"*70)\n    ","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## fillna the outliers :","metadata":{}},{"cell_type":"code","source":"def replace_outliers(df):\n    quantiles = df.quantile([0.25, 0.75])\n    \n    # Calculate lower and upper bounds\n    lower_bound = quantiles[0.25] - (quantiles[0.75] - quantiles[0.25]) * 1.5\n    upper_bound = quantiles[0.75] + (quantiles[0.75] - quantiles[0.25]) * 1.5\n    \n    # replace outliers with minimum or maximum values\n    df = df.apply(lambda x: np.where((x < lower_bound), np.min(x), \n                                      np.where(x > upper_bound, np.max(x), x)))\n    \n    return df\n\n# Fillna outliers in this features:\n\ndf_cleaned_without_target[\"Rainfall\"]=replace_outliers(df_cleaned_without_target[\"Rainfall\"])\ndf_cleaned_without_target[\"Visibility\"]=replace_outliers(df_cleaned_without_target[\"Visibility\"])\ndf_cleaned_without_target[\"Wind_speed\"]=replace_outliers(df_cleaned_without_target[\"Wind_speed\"])\ndf_cleaned_without_target[\"Humidity\"]=replace_outliers(df_cleaned_without_target[\"Humidity\"]) \nprint(\"Data after cleand Data & fillna outliers :\",np.shape(df_cleaned_without_target))\nprint(\"Data after only Remove outliers :\",np.shape(df))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization :","metadata":{}},{"cell_type":"code","source":"color_palette = ['#8B008B', '#1E90FF', '#FF6347', '#B22222', '#228B22', '#2F4F4F']\n# Rates of Humidity aat each season :   :\n\nseason = df.groupby(\"Seasons\")[\"Humidity\"].mean().sort_values(ascending=False)\n\n# Display the rates of humidity at every season :\n\nprint(\"Rates Humidity at each season :\\n\",season)\n\nseason_index = season.reset_index()\nfig = px.pie(season_index, names='Seasons', values='Humidity', color_discrete_sequence=color_palette)\n\nfig.update_layout(\n    title=\"Observation seasons .vs Humidity\",\n    legend_title=\"Seasons\",\n    width=800,  \n    height=600  \n         )\n# Show the plot\nfig.show()\n#################################################\nprint(\"-\"*70)\n\n# Rates of Temperature at each season :   :\n\nseason1 = df.groupby(\"Seasons\")[\"Temperature\"].mean().sort_values(ascending=False)\n\n# Display the rates of humidity at every season :\n\nprint(\"Rates Temperature at each season :\\n\",season1)\n\nseason_index = season1.reset_index()\nfig = px.pie(season_index, names='Seasons', values='Temperature', color_discrete_sequence=color_palette)\n\nfig.update_layout(\n    title=\"Observation seasons .vs Temprature\",\n    legend_title=\"Seasons\",\n    width=800,  \n    height=600  \n         )\n# Show the plot\nfig.show()\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rates of Rainfall at each season :   :\n\nseason2 = df.groupby(\"Seasons\")[\"Rainfall\"].mean().sort_values(ascending=False)\n\n# Display the rates of Rainfall at every season :\n\nprint(\"Rates Snowfall at each season :\\n\",season2)\n\nseason_index = season2.reset_index()\nfig = px.pie(season_index, names='Seasons', values='Rainfall', color_discrete_sequence=color_palette)\n\nfig.update_layout(\n    title=\"Observation seasons .vs Rainfall\",\n    legend_title=\"Seasons\",\n    width=800,  \n    height=600  \n         )\n# Show the plot\nfig.show()\nprint(\"-\"*70)\n#####################################################\n\n# Calculate mean visibility for each season\nseason3 = df.groupby(\"Seasons\")[\"Visibility\"].mean().sort_values(ascending=False)\n\n\nprint(\"Rates of Visibility at each season:\\n\", season3)\n\n\nseason_index = season3.reset_index()\n\n# Create a pie chart\nfig = px.pie(season_index, names='Seasons', values='Visibility', color_discrete_sequence=px.colors.sequential.Plasma)\n\n# Update layout for the pie chart\nfig.update_layout(\n    title=\"Observation Seasons vs. Visibility\",\n    legend_title=\"Seasons\",\n    width=800,  \n    height=600  \n)\n\n# Show the plot\nfig.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot the Holiday observation :\n\nholiday = df['Holiday'].value_counts()\nplt.figure(figsize=(12, 6))\nsns.barplot(x=holiday.index, y=holiday.values,hue=holiday.index,palette=\"cool\")\nplt.title('holidayes observations :',fontdict=f1)\nplt.xlabel('holiday(True /False)',fontdict=f1)\nplt.ylabel('Number of content ',fontdict=f1)\nplt.grid(True, axis='y')\nplt.xticks(rotation=45,color=\"b\")\nplt.yticks(rotation=-45,color=\"b\")\nplt.show()\n\n# make a bar plot for Functioning Day :\n\nfunc_day = df['Functioning Day'].value_counts()\nplt.figure(figsize=(12, 6))\nsns.barplot(x=func_day.index, y=func_day.values,hue=func_day.index,palette=\"Reds\")\nplt.title('Functional day observations : ',fontdict=f1)\nplt.xlabel('Functioning day (Yes/No)',fontdict=f1)\nplt.ylabel('Number of content',fontdict=f1)\nplt.grid(True, axis='y')\nplt.xticks(rotation=45,color=\"b\")\nplt.yticks(rotation=-45,color=\"b\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Observation  between number of rential hours & Visibility  :\nprint(\"Positive Relationship :\")\nfig = px.scatter(df, x='Visibility', y='y', color='y', color_discrete_sequence=color_palette,trendline='ols')\nfig.update_layout(title=\"number of rential hours & Visibility \", xaxis_title=\"Visibility\", yaxis_title=\"y\")\nfig.show()\n\n###################################################################################################\n\n# Observation  Wind_speed vs.y (scatterplot)  :\nprint(\"Positive Relationship :\")\nfig = px.scatter(df, x='Wind_speed', y='y', color='y', color_discrete_sequence=color_palette,trendline='ols')\nfig.update_layout(title=\"number of rential hours & Wind_speed \", xaxis_title=\"Wind_speed\", yaxis_title=\"y\")\nfig.show()\n\n\n#######################################################################################################\n\n# Observation  Visibility vs.Snowfall(cm) (scatterplot)  :\n\nprint(\"Negative Relationship :\")\nfig = px.scatter(df, x='Visibility', y='Snowfall(cm)', color='Snowfall(cm)', color_discrete_sequence=color_palette,trendline='ols')\nfig.update_layout(title=\"Visibility vs.Snowfall(cm)\", xaxis_title=\"Visibility\", yaxis_title=\"Snowfall\")\nfig.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Waffle Visualization :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<div style=\"border: 2px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p> &bull; i Make Waffle observation to see which feature that Controlled on other Features . </p> \n\n<p> &bull; i found ID Feature controled on other features .</p> \n\n<p> &bull; Finally , PRICE Feature : is the most important Feature in the Data   .</p> \n\n\n</div>","metadata":{}},{"cell_type":"code","source":"# Make a waffel for the Data:\n# from this waffle i see which Feature is a controled on other features : \n\n\nf2 = {\"family\": \"serif\", \"size\": 12, \"color\": \"r\"}\n\nfrom pywaffle import Waffle\n\nnumeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\nvalues = data[numeric_columns].sum().tolist()\n\nfig = plt.figure(\n    FigureClass=Waffle,\n    rows=100,\n    columns=35,\n    values=values,\n    legend={'labels': numeric_columns.tolist(),\n            'loc': \"upper left\",\n            'bbox_to_anchor': (1, 1)},\n    figsize=(25, 25)\n)\nplt.title(\"Waffle observation on Data\",fontdict=f2)\nplt.suptitle(\"ID feature control on all other features \",fontdict=f2)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## By clicking Double click on any Button , you will get the info about this condation","metadata":{}},{"cell_type":"code","source":"# Histogram that show how can see Temperature with Holiday  :\nfig = px.histogram(df, x=\"Temperature\", color=\"Holiday\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Temperature .vs Holiday\",\n    legend_title=\"Holiday\",\n    width=800,  \n    height=600  \n)\nfig.show()\n#############################################################\n# Histogram that show how can see Rainfall with Holiday  :\nfig = px.histogram(df, x=\"Rainfall\", color=\"Holiday\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Rainfall .vs Holiday\",\n    legend_title=\"Holiday\",\n    width=800,  \n    height=600  \n)\nfig.show()\n\n##################################################################\n\n# Here also you can see how Wind_speed feature liked with Date \n\nfig = px.histogram(df, x=\"Wind_speed\", color=\"Date\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Wind_speed .vs Date\",\n    legend_title=\"Date\",\n    width=800,  \n    height=600  \n)\nfig.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here also you can see how Rainfall feature liked with Month \n\nfig = px.histogram(df, x=\"Rainfall\", color=\"Month\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Rainfall .vs Month\",\n    legend_title=\"Month\",\n    width=800,  \n    height=600  \n)\nfig.show()\n\n######################################################\n\n# Here also you can see how Rainfall feature liked with Days \n\nfig = px.histogram(df, x=\"Rainfall\", color=\"Day\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Rainfall .vs Dyas\",\n    legend_title=\"Day\",\n    width=800,  \n    height=600  \n)\nfig.show()\n\n###############################################################\n\n# Here also you can see how Rainfall feature liked with  Functioning Day\n\nfig = px.histogram(df, x=\"Rainfall\", color=\"Functioning Day\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Rainfall .vs Functioning Day\",\n    legend_title=\"Functioning Day\",\n    width=800,  \n    height=600  \n)\nfig.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# whats the largest year that Rainfall is the biggest ?\n\nfig = px.histogram(df, x=\"Rainfall\", color=\"Year\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Rainfall .vs Year\",\n    legend_title=\"Year\",\n    width=800,  \n    height=600  \n)\nfig.show()\n####################################################################\n\n# in which hour was the largest Rentail Bikes ?\nfig = px.histogram(df, x=\"y\", color=\"Hour\")\nfig.update_layout(\n    bargap=0.2,\n    title=\"Observation Rentail Bikes .vs Hour\",\n    legend_title=\"Hour\",\n    width=800,  \n    height=600  \n)\nfig.show()\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# whats the affect wheather on Holidays ?\n# which Holiday are the most Solar_Radiation & Rainfall & Wind_speed :\n\nx = df[\"Holiday\"]\ny1 =df[\"Rainfall\"]\ny2 = df[\"Wind_speed\"]\ny3=df[\"Solar_Radiation\"]\n    \nfig = go.Figure(data=[\n        go.Bar(name='Rainfall', x=x, y=y1,marker_color=\"blue\"),\n        go.Bar(name='Wind_speed', x=x, y=y2,marker_color=\"red\"),\n        go.Bar(name='Solar_Radiation',x=x,y=y3,marker_color=\"yellow\")\n    ])\n    \nfig.update_layout(barmode='group',\n                      title='Holiday . vs Rainfall & Hummidity & Solar_Radiation ',\n                      legend_title=\"weather\",\n                      width=800,  \n                      height=600 ,       \n                      xaxis_title=\"Holiday\",\n                      yaxis_title=\"Frequency\",\n                      xaxis_tickangle=-45,\n                      yaxis_tickangle=45\n    ) \n    \nfig.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# whats The Relations Between Seasons & Wheather Condations ?\n\nx = df[\"Seasons\"]\n\ny1 =df[\"Rainfall\"]\ny2 = df[\"Humidity\"]\ny3=df[\"Solar_Radiation\"]\ny4=df[\"Wind_speed\"]\ny5=df[\"Temperature\"]\n\nfig = go.Figure(data=[\n        go.Bar(name='Rainfall', x=x, y=y1,marker_color=\"blue\"),\n        go.Bar(name='Humidity', x=x, y=y2,marker_color=\"red\"),\n        go.Bar(name='Solar_Radiation', x=x, y=y3,marker_color=\"pink\"),\n        go.Bar(name='Wind_speed', x=x, y=y4,marker_color=\"yellow\"),\n        go.Bar(name='Temperature', x=x, y=y5,marker_color=\"green\")\n        ])\n    \nfig.update_layout(barmode='group',\n                      title='affects of wheather on Seasnos ',\n                      legend_title=\"Wheater\",\n                      width=800,  \n                      height=600 ,       \n                      xaxis_title=\"Seasons\",\n                      yaxis_title=\"Frequency\",\n                      xaxis_tickangle=-45,\n                      yaxis_tickangle=45\n    ) \n    \nfig.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create scatter between Months & Number of Bikes that rentals :\nx = df[\"Month\"]\ny = df[\"y\"]\n    \nfig = go.Figure(data=[go.Scatter(x=x, y=y)])\n    \n# Update layout for better appearance\nfig.update_layout(title='scatter plot between Months & Number of Bikes that rentals',\n                      xaxis_title='Months',\n                      yaxis_title='Number of Bikes that rentals per Hour')\n    \n# Display the plot\nfig.show()\n\n############################################################################\n\n\n# Create scatter plot between Visibility & snowfall  :\nx = df[\"Visibility\"]\ny = df[\"Snowfall(cm)\"]\n    \nfig = go.Figure(data=[go.Scatter(x=x, y=y)])\n    \n# Update layout for better appearance\nfig.update_layout(title='scatter plot between Visibility & snowfall ',\n                      xaxis_title='Visibility',\n                      yaxis_title='Snowfall(cm)')\n    \n# Display the plot\nfig.show()\n\n\n\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot the histgram:\ndf.hist(figsize=(25,25),color=\"b\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# observe for the float_data\nplt.figure(figsize=(25, 25), dpi=250)\nsns.set(style=\"whitegrid\")  # Set the plot style\nsns.set_palette(\"coolwarm\")    # Set the color palette\nsns.pairplot(df.select_dtypes(include='float64'), plot_kws={'alpha': 0.6, 's': 80})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer Categorical Data into Numerical :","metadata":{}},{"cell_type":"code","source":"df=pd.get_dummies(df)\ndf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split Data for Regression :\n\n\n<div style=\"border: 5px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p> &bull; i split data into x_reg & y_cly_regass </p> \n\n<p> &bull; x_reg : refer into all Data Except Target (y)</p> \n\n<p> &bull; y_reg : refer into Target (y)</p> \n\n<p> &bull; then , iused train_test_split method to divide data into test , train</p> \n</div>\n","metadata":{}},{"cell_type":"code","source":"# train split of the Data\nx_train,x_test,y_train,y_test=train_test_split(x_reg,y_reg,test_size=0.3,random_state=42)\nprint(\"x_train shape : \",x_train.shape)\nprint(\"x_test shape : \",x_test.shape)\nprint(\"y_train shape : \",y_train.shape)\nprint(\"y_test shape : \",y_test.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer Categorical Data into Numerical Data ","metadata":{}},{"cell_type":"code","source":"df=pd.get_dummies(df)\ndf","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Processing :\n\n\n\n\n\n\n\n<div style=\"border: 5px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n<p>&bull; I used pipline method to make strong structure for processing for both scaler , labelEncoder & Onehot  </p>\n<p> &bull; i used OneHotEncoder  method from to convert any object/Categorical data into Numerical </p> \n\n<p> &bull; Also i used Standard Scaler on this Data</p> \n\n","metadata":{}},{"cell_type":"markdown","source":"## Create processing pipeline :","metadata":{}},{"cell_type":"code","source":"# Define the numeric features and categorical features\ncategorical_features =x_train.select_dtypes(\"object\").columns\nnumeric_features = x_train.select_dtypes(\"number\").columns\n\n# Print the features to ensure they are correctly identified\nprint(\"Categorical Features:\", categorical_features)\nprint(\"Numeric Features:\", numeric_features)\n\n# Create preprocessing pipelines for both numeric and categorical data\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bulding Regression models :\n\n<div style=\"border: 2px solid yellow; padding: 10px; background-color: #F5F5F5; color: black; border-radius: 30px 45px 30px 45px;\">\n\n\n\n<p> &bull; RandomForestRegressor  </p> \n\n<p> &bull; DecisionTreeRegressor  </p> \n\n<p> &bull; RidgeRegression  </p> \n\n<p> &bull; gradient boosting regressor  </p>\n\n<p> &bull; Polynimial Regression   </p>\n\n<p> &bull; Linear Regression   </p>\n\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"## Define the models :\n\nRG = Ridge(alpha=0.1)\nDT=DecisionTreeRegressor(max_depth=9,max_features=10,random_state=42)\npoly = PolynomialFeatures(degree=2)\nLR=LinearRegression()\n\ngridsearch1=GridSearchCV(estimator=RandomForestRegressor(),\n                        param_grid={\"n_estimators\":[70,100,250,300],\n                        \"max_depth\":[50,120,200,300]},\n                         cv=3,\n                         return_train_score=False,\n                         scoring='r2')\n\ngridsearch2=GridSearchCV(estimator=GradientBoostingRegressor(),\n                        param_grid={\"n_estimators\":[20,50,80,120],\n                        \"max_depth\":[50,90,120,200]},\n                         cv=3,\n                         return_train_score=False,\n                         scoring='r2')\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the pipelines for each model\n\n\npipeline_random_forest = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('random_forest',gridsearch1 )\n])\n\npipeline_decision_tree = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('decision_tree', DT)\n])\n\n\npipeline_gradient = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('gridsearch',gridsearch2)\n])\n\npipeline_poly_linear=Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('polynomial',poly),\n    ('linear',LR)\n])\n\npipeline_poly_Ridge=Pipeline(steps=[\n    ('preprocessor', preprocessor),\n     ('polynomial',poly),\n    ('Ridge',RG)\n])\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the models using the pipelines\n\npipeline_random_forest.fit(x_train, y_train)\npipeline_decision_tree.fit(x_train, y_train)\npipeline_poly_Ridge.fit(x_train, y_train)\npipeline_gradient.fit(x_train, y_train)\npipeline_poly_linear.fit(x_train,y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"best parameters for GradientBoosting:\",pipeline_gradient.best_estimator_)\nprint(\"best parameters for GradientBoosting:\",pipeline_random_forest.best_estimator_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to evaluate a regression pipeline & to check if found overfitting :\n\ndef evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test, sc_y=None):\n    y_pred = pipeline.predict(X_test)\n    y_train_pred = pipeline.predict(X_train)\n    \n    r2_train = r2_score(y_train, y_train_pred)\n    r2_test = r2_score(y_test, y_pred)\n    acc=pipeline.score(X_test,y_test)\n    \n    mse_train = mean_squared_error(y_train, y_train_pred)\n    mse_test = mean_squared_error(y_test, y_pred)\n    mse_model=mean_squared_error(y_test,y_pred)\n    \n    if sc_y is not None:\n        y_pred = sc_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train_pred = sc_y.inverse_transform(y_train_pred.reshape(-1, 1))\n        \n        r2_test = r2_score(sc_y.inverse_transform(y_test.reshape(-1, 1)), y_pred)\n        mse_test = mean_squared_error(sc_y.inverse_transform(y_test.reshape(-1, 1)), y_pred)\n    \n    print(f\"Model: {pipeline.steps[-1][1]}\")\n    print(\"R2 Train Score:\", r2_train)\n    print(\"R2 Test Score:\", r2_test)\n    print(\"-\"*50)\n    print(\"Mean Squared Error of Train:\", mse_train)\n    print(\"Mean Squared Error of Test:\", mse_test)\n    print(\"-\"*50)\n    print(\"Model accuracy:\",acc)\n    print(\"Model_Mean_square_Error:\",mse_model)\n    \n    # Plot the predicted vs true values\n    plt.figure(figsize=(7, 5))\n    plt.scatter(y_test, y_pred, alpha=0.5)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title(f'Predicted vs True Values for {pipeline.steps[-1][0]} Model')\n    plt.show()\n    \n    # Plot the residuals\n    plt.figure(figsize=(7, 5))\n    sns.histplot((y_test - y_pred), bins=20, kde=True)\n    plt.xlabel('Residuals (True - Predicted)')\n    plt.ylabel('Frequency')\n    plt.title(f'Residuals for {pipeline.steps[-1][0]} Model')\n    plt.show()\n\n\n\nevaluate_pipeline(pipeline_random_forest, x_train, x_test, y_train, y_test)\nevaluate_pipeline(pipeline_decision_tree, x_train, x_test, y_train, y_test)\nevaluate_pipeline(pipeline_gradient, x_train, x_test, y_train, y_test)\nevaluate_pipeline(pipeline_poly_linear, x_train, x_test, y_train, y_test)\nevaluate_pipeline(pipeline_poly_Ridge, x_train, x_test, y_train, y_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Access the decision tree regressor from the pipeline\nDT = pipeline_decision_tree.named_steps['decision_tree']\nfeature_importances = DT.feature_importances_\n# Create a Series with feature importances\nimportance_series = pd.Series(feature_importances, index=x_reg.columns).sort_values(ascending=False)\n\nprint(\"Feature Importances:\\n\", importance_series)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\ntree_data = export_graphviz(DT, filled=True, rounded=True, feature_names=x_reg.columns, out_file=None)\ngraphviz.Source(tree_data)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<div style=\"border: 5px solid green; padding: 10px; background-color: #F5F5F5; color: #4933ff; border-radius: 30px 45px 30px 45px;\">\n\n\n    \n<p>&bull; in the Conclusion i want to thank all team of Neuro & Especially Eng\\Taher .</p>\n<p>&bull; thanks for all Neuro Team .</p>\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRS7ljsKnRCGIkKlSRUEC4LsQym9eQNPFnvwA&s\" alt=\"My Image\">\n\n<p>&bull; Your support is greatly appreciated! .</p>\n\n\n<p>&bull; Kaggle: Your Home for Data Science</p>\n\n\n<p>&bull; <a href=\"https://www.kaggle.com/abdalrhamnhebishy\" target=\"_blank\">Kaggle profile</a>.</p>\n\n\n<p>&bull;  AbdalRhman Hebishy , DataScientist||Machine learning||Deeplearning Engineer .</p>\n   \n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<img src=\"Neuro.jpg\" alt=\"My Image\">","metadata":{}}]}